\documentclass[12pt] {article}
\usepackage{../template/NotesTeX}
\begin{document}

\title{Mathematical Modelling Notes}
\author{Alexander Bailey}
\maketitle

\section{Modelling Process}
\subsection*{Factors}
Anything that will have some effect on the your calculations e.g. drag, friction, mass, buoyancy, area.
Note that separate things qualities will count separately e.g. a triangle's area and a square's.

\subsection*{Assumptions}
A statement that makes the problem simpler - cancels factors. 
e.g. 'Total length of wood is not reduced when it is cut'
or 'There are no significant currents' 

\subsection*{Precise Problem Statement}
Given (key factors, assumptions and values needed to solve), Find (the value you're asked to find)

\subsection*{Formulating a Model}
\begin{equation*}
x \propto y,1/z \implies x = \frac{ky}{z} 
\end{equation*}

\newpage
\subsection*{Modelling Forces}
Use Newton's 2nd law, subtract negative forces and add positive ones. These are some common models:

\begin{center}
\begin{tabular}{c|c}
  Force & Model \\
  Gravity & $f_g = mg$ \\
  Spring & $f_s = -kx$ \\
  Friction & $f_f = \mu f_n $ \\
  Buoyancy & $f_b = \rho V g$ \\
  Drag Force & $f_d = -cv $ \\
  Drag Force & $f_d = cv^2 $ \\
\end{tabular}
\end{center}

\subsection*{Dimensions}
By performing dimensional analysis, we can produce a model or determine the dimension of a property. Some common dimensions include,

\begin{center}
\begin{tabular}{c|l}
  Length & $L$ \\
  Mass & $M$ \\
  Time & $T$ \\
  Temperature & $\theta$ \\
  Area & $L^2$ \\
  Density & $\frac{M}{L^3}$ \\
  Force & $\frac{ML}{T^2}$ \\
  Energy & $\frac{ML^2}{T^2}$ \\
\end{tabular}
\end{center}

\section{Probability}
\subsection*{Some General Rules}
\begin{align*}
  Pr(A \vert B) &= \text{Probability of A happening given B has already happened} \\
                &= \frac{Pr(A\&B)}{Pr(B)} \\
  Pr(A \& B) &= Pr(A) \times Pr(B) \text{ If they are independent}
\end{align*}
\subsection*{Continuous Random Variables}
Continuous Random Variables that are infinite i.e height as there are an infinite amount 
of possible heights with infinite accuracy. When graphed the area under a graph (probability density function)
will always sum to one i.e
\begin{equation*}
  \int_\infty^\infty f(x)dx = 1 
\end{equation*}

\subsection*{PDFs and CDFs}
The probability density function (PDF) is the density of a continuous random variable, it provides
a value for a `probability' that the random variable would equal a given sample. The integral
of a PDF is the cumulative distribution function (CDF) gives the probability for less than or greater than
the given sample.

\subsection*{Frequency/Probablity Trees}
A probability tree is a tree in which you label edges with the probability of an event
and label vertices with each event. A frequency tree is very similar but you multiply
those probabilities by some fixed amount i.e 1000 people etc. These will carry through so you
multiply the number in the parent vertex by the amount on that edge. 

\subsection*{Bayes Theorem}
\begin{equation*}
  Pr(A \vert B) = \frac{Pr(A)}{Pr(A)Pr(B \vert A) + Pr(~A)Pr(B \vert ~A)}
\end{equation*}

\section{Differentiation}
\subsection*{Series and Approximation}
These infinite series allow you to approximate any function for as many or as few terms as you like.
It can be important for these questions to remember that `linear' means 2 terms, `quadratic' means 3 terms and `cubic' means 4.
\subsubsection*{Maclaurin Series}
The Maclaurin Series is an approximation that is accurate around $x=0$, it is a special case of the Taylor Series.
\begin{equation*}
  f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x^2+\frac{f'''(x)}{3!}x^3 \dotsc
\end{equation*}
\subsubsection*{Taylor Series}
The Taylor Series is an approximation that is accurate around $x=a$.
\begin{equation*}
  f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2}(x-a)^2 + \frac{f'''(0)}{3!}(x-a)^3 \dotsc
\end{equation*}
\subsubsection*{Binomial Series}
You will likely have to manipulate/factor your expression to get into the binomial form.
\begin{equation*}
  (1+x)^n = 1 + \frac{n}{1!}x + \frac{n(n-1)}{2!}x^2 + \frac{n(n-1)(n-2)}{3!}x^3 \dotsc
\end{equation*}
\subsection*{Implicit Differentiation}
Implicit Differentiation is simply differentiating term by term for implicit equations.
That is, equations that have both $x$ and $y$ on either side of the equation. 
\begin{example}
\begin{gather*}
  \text{Finding } \frac{dy}{dx} \text{ for } x^2-2x+2y^3=1 \\
  2x\frac{dx}{dx} -2\frac{dx}{dx} +6y^2\frac{dy}{dx}=0 \\
  \implies \frac{dy}{dx} = \frac{1-x}{3y^2}
\end{gather*}
\end{example}
\subsection*{Numerical Differentiation}
Numerical Differentiation essentially comes down to manipulation of the definition of a derivative. Mostly using these 4 equations:
\begin{equation*}
  f'(x) \approx \frac{f(x+h)-f(x)}{h} \text{ (first forward difference)}
\end{equation*}
\begin{equation*}
  f'(x) \approx \frac{f(x)-f(x-h)}{h} \text{ (first backward difference)}
\end{equation*}
\begin{equation*}
  f'(x) \approx \frac{f(x+h)-f(x-h)}{h} \text{ (first central difference)}
\end{equation*}
\begin{equation*}
  f''(x) \approx \frac{f(x+h)-2f(x)+f(x-h)}{h^2} \text{ (second central difference)}
\end{equation*}
\section{Integration}
\subsection*{Integration Techniques}
\begin{enumerate}
  \item $u$ substitution $dx=du\dotsc$
  \item Integration by Parts $\int u \frac{dv}{dx} dx = uv - \int v \frac{du}{dx}dx$
  \item Partial Fractions
\end{enumerate}
\subsection*{Partial Fractions}
Partial Fractions is a method to split up rational functions into easy to integrate functions. 
It comes in 3 different forms:
\subsubsection*{Simple Form}
\begin{equation*}
\frac{p(x)}{(x-a)(x-b)(x-c)\dotsc} = \frac{A}{x-a} + \frac{B}{x-b} + \frac{C}{x-c} + \dotsc
\end{equation*}
\subsubsection*{Repeated Factors}
\begin{equation*}
  \frac{p(x)}{(x-a)(x-b)^3\dotsc} = \frac{A}{x-a} + \frac{B_1}{x-b} + \frac{B_2}{(x-b)^2} + \frac{B_3}{(x-b)^3} \dotsc
\end{equation*}
\subsubsection*{Irreducible quadratic factors}
\begin{equation*}
  \frac{p(x)}{(x-a)(x^2+bx+c)} = \frac{A}{x-a} + \frac{Bx+c}{x^2+bx+c} + \dotsc
\end{equation*}
\subsubsection*{The cover-up rule}
When the denominator of the split up fraction is of the form $x-a$ you can `cover-up' that factor in the denominator
and then plugin the $x$ value from $x-a=0$ and solve. This will give you the `A' value.

\section{Ordinary Differential Equations}
Ordinary differential equations are equations containing one or more functions of one independent variable. 
You can recognise an ODE from a PDE (partial differential equation) because a PDE will contain $\partial$ (pronounced 'del')
and ODEs have standard 'd'. $\frac{dy}{dx}$ means $y$ is the dependent variable and $x$ is the independent variable. $\frac{dx}{dt}$ $x$ is dependent, $t$ is independent.

\subsection*{Properties}
\subsubsection*{Order}
Highest derivative (also equal to number of values needed to find a particular solution) e.g.
\begin{align*}
    &\frac{dy}{dx} = 5x \text{ is 1st Order} \\
    &\frac{d^4y}{dx^4} = \frac{dy}{dx} + 2 \text{ is 4th Order} \\
\end{align*}

\subsubsection*{Linear}
Involves only derivatives of y and terms of y to the 1st power e.g. ONLY $\frac{dy}{dx}$, $y$ etc.
\begin{align*}
    &\frac{d^4y}{dx^4} + \frac{dy}{dx} = 2 \text{ is linear} \\
    &\frac{dy}{dx} = 2y + 3 \text{ is linear} \\
\end{align*}

\subsubsection*{Homogeneity} 
If all (non-zero) terms involve the dependent variable then the equation is homogeneous
\begin{align*}
    &\frac{dx}{dt} = x \text{ is homogeneous} \\
    &\frac{dy}{dx} = 2y + 3 \text{ is not homogeneous (3 doesn't involve x)} \\
\end{align*}

\subsection*{Forming Differential Equations}
In typical exam questions there are few points at which you will form a differential equation: modelling a set 
of forces in the typical modelling questions, using proportionality or previous knowledge. Typically the modelling 
questions will use Newton's 2nd law which states $\sum F = ma$ and then you can sum the forces and use it to find
mass/acceleration (or their derivatives). 

\subsection*{Solving Differential Equations}
\begin{enumerate} 
    \item Direct Integration
    \item Separation of Variables 
    \item Euler's Method    
    \item Integrating Factor
    \item Exponential Substitution
\end{enumerate}

\subsubsection*{Euler's Method}
Euler's method is a numerical method for solving D.E.s that uses a 2 term Taylor Series 
and is simply stated by the equation.
\begin{equation*}
  f(x+h) \approx f(x) + hf'(x)
\end{equation*}
These questions are easiest to complete if you use a table for the values 
of $x$, $f(x)$, $f'(x)$ and then calculate $f(x+h)$ e.g.
\vspace{1em}
\newline
$\begin{array}{c|c|c|c}
  x & f(x) & f'(x) & f(x) + hf'(x) \\
  1 & 1^2 & 2 & 4 \\
  4 & \dots & \dots & \dots \\
\end{array}$

\subsubsection*{Exponential Substitution}
Exponential Substitution allows you to solve a 2nd order O.D.E using something called
a `characteristic equation'. You substitute in a trial solution $e^{\lambda t}$
and then complete the substitution (differentiate) and then divide through by your 
trial solution leaving you with the characteristic equation which you can then solve for
$\lambda$ (note: if both solutions are negative, it is called `stable'). You then substitute your values into the equation:
\begin{equation*}
  x=C_1e^{\lambda t} + C_2e^{\lambda t} \\
\end{equation*}
This is a general solution, to calculate the exact solution you need two points 
(for a 2nd order equation) to calculate values of $C_1$ and $C_2$. e.g
\begin{example}
\begin{gather*}
  \frac{d^2x}{dt^2} - \frac{dx}{dt} -6x =0 \\
  \lambda^2 - \lambda - 6 = 0\\
  \implies \lambda=3,-2 \\
  x=c_1e^{3t} + c_2e^{-2t} \text{ gen. soln.} \\
  \text{Given x(0) = 0, x'(0)=5} \\
  0 = C_1 + C_2 \implies C_1 = -C_2 \\
  5 = 3C_1 + -2C_2 \implies C_2 = -1 \\
  \implies C_1 = 1 \\
  x=e^{3t} - e^{-2t} \text{ exact soln.} \\
\end{gather*}
\end{example}

\subsubsection*{Integrating Factor}
\begin{equation*}
\text{if } h(x)=\int p(x)dx
\end{equation*}
\begin{equation*}
  \frac{dy}{dx} + p(x)y = r(x) \text{ has solution } y = e^{-h(x)} \int e^{h(x)}r(x)dx
\end{equation*}

\newpage

\section{Vectors}
Vectors are a special case of matrices that only involve one column (for column vectors)
or one row (row vectors). It is usually described as a directed length or arrow i.e 
it has both direction and magnitude! Vectors can be written in a number of ways:
\begin{gather*}
  a\hat{\imath} + b\hat{\jmath} + c\hat{k} \\
  \begin{bmatrix} 
    a \\
    b \\ 
    c \\
  \end{bmatrix} \\
  \begin{bmatrix}
    a &
    b & 
    c &
  \end{bmatrix} 
\end{gather*}

The magnitude (length) of a vector is given by 
\begin{equation*}
  \sqrt{a^2+b^2+c^2}
\end{equation*}
The dot product of two vectors is described by the formula
\begin{equation*}
  \begin{bmatrix}
    a \\
    b \\ 
    c \\
  \end{bmatrix} \cdot
  \begin{bmatrix}
    d \\
    e \\ 
    f \\
  \end{bmatrix}
  = a\times d + b\times e + c \times f 
\end{equation*}
Note that this formula works for vectors of all sizes. The dot product can also be defined 
geometrically as $|a||b|\cos\theta$ where a and b are the two input vectors.

The cross product of two vectors is given by 
\begin{equation*}
  \begin{bmatrix}
    a \\
    b \\ 
    c \\
  \end{bmatrix} \times 
  \begin{bmatrix}
    p \\
    q \\ 
    r \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    br-cq \\
    -(ar-cp) \\ 
    aq-bp \\
  \end{bmatrix}
\end{equation*}
The cross product returns a vector that is normal to both of the input vectors. It has a magnitude 
$|a||b|\sin\theta$ where a and b are the two input vectors.

The projection of u onto v (the amount u in the direction of v) is given by
\begin{equation*}
  (\frac{u\cdot v}{v\cdot v})v
\end{equation*}

\marginnote{The lack of any notated operator means it is scalar multiplication}

\newpage

\section{Matrices}
Vectors are just a special case of Matrices (one column/row) so a lot of their properties
are shared with matrices. For instance, addition/subtraction are performed component wise 
and scalar multiplication works just like it does in vectors i.e
\begin{equation*}
  \begin{bmatrix}
    a & d \\
    b & e \\
    c & f \\
  \end{bmatrix}
  + 
  \begin{bmatrix}
    g & j \\
    h & k \\
    i & l \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    a+g & d+j \\
    b+h & e+k \\
    c+i & f+l \\
  \end{bmatrix}
\end{equation*}
\begin{equation*}
  3
  \begin{bmatrix}
    -1 & 0 \\
    2 & 1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    -3 & 0 \\
    6 & 3 \\
  \end{bmatrix}
\end{equation*}
The only thing you will need to keep in mind for matrix addition/subtraction is 
that you can only add matrices of the same order. What's order?

\subsection*{Order of Matrices}
By convention, the order of a matrix is expressed like $m \times n$. Where m is the number
of rows and n the number of columns. This is opposite to the traditional `$x$ \& $y$' way
of thinking but you'll see why when you get to multiplication.

\subsection*{Transpose}
The first of the new operations we will learn for matrices is something called `transpose'.
You can think of this like the rotation of a matrix. It is represented by a superscript capital T.
\begin{equation*}
  \begin{bmatrix} 
    1 \\
    2 \\
    1 \\
  \end{bmatrix} 
  ^\intercal
  =
  \begin{bmatrix}
    1 & 2 & 1 
  \end{bmatrix}
\end{equation*}
\begin{equation*}
  \begin{bmatrix} 
    0 & -2 \\
    2 & 12 \\
    0 & -3 \\
  \end{bmatrix} 
  ^\intercal
  =
  \begin{bmatrix} 
    -2 & 12 & 3 \\
     0 & 2 & 0 \\
  \end{bmatrix} 
\end{equation*}
Note: $\textbf{A}\textbf{B} \neq \textbf{B}\textbf{A}$ but $(\textbf{A}\textbf{B})^\intercal = \textbf{B}^\intercal \textbf{A}^\intercal$
\newpage

\subsection*{Matrix Multiplication}
For two $m \times n$ matrices, 
\begin{itemize}
  \item Cover everything but the first column in the second matrix
  \item Take the dot product (just like vectors) of that and each of the m rows of the first matrix (where the first is the first row of the matrix output, the second the second row etc)
  \item Repeat for each of the $n$ columns in the second matrix
\end{itemize}
For two $m \times n$ matrices, the first must have the same number of rows as columns in the
second. 

\subsubsection*{Example}
\begin{gather*}
  \begin{bmatrix}
    1 & 3 & 1 \\
    2 & 4 & 5 \\
    6 & 1 & 2 \\
  \end{bmatrix}
  \begin{bmatrix}
    2 & 3 & 1 \\
    4 & 2 & 1 \\
    6 & 1 & 1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    20 & 10 & 5 \\
    50 & 19 & 11 \\
    28 & 22 & 9 \\
  \end{bmatrix}
  \\
  \begin{bmatrix}
    [1 & 3 & 1] \\
    [2 & 4 & 5] \\
    [6 & 1 & 2] \\
  \end{bmatrix}
  \begin{bmatrix}
    2 & 3 & 1 \\
    4 & 2 & 1 \\
    6 & 1 & 1 \\
  \end{bmatrix} 
  \\
  [2, 4, 6]\cdot [1, 3, 1] = 2\cdot1 + 4\cdot3 + 6\cdot1 = 20
\end{gather*}

\subsection*{Matrix Multiplication as a Transformation}
Taking two equations, that move two points $x$ and $y$ to two new points $x_{new}$ and $y_{new}$. 
\begin{align*}
  2x - y = x_{new} \\
  x + y = y_{new}
\end{align*}
These can be represented as matrices, (the matrix containing the co-efficients
is called a `transformation matrix')
\begin{equation*}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1 
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y 
  \end{bmatrix}
  =
  \begin{bmatrix}
    x_{new} \\
    y_{new}
  \end{bmatrix}
\end{equation*}

\newpage

\begin{definition}

\subsection*{Transformation Matrices }
A transformation matrix is a square matrix that when you multiply a matrix of points by it 
will perform a transformation i.e scale it or rotate it etc. All of these examples are for 
a `unit square' which for a length k is the matrix:
\begin{equation*}
  \begin{bmatrix}
    k & 0 \\
    0 & k \\
  \end{bmatrix}
\end{equation*}

\subsubsection*{Stretch of $k$ in the $x$ direction}
\begin{equation*}
  \begin{bmatrix}
    k & 0 \\
    0 & 1 \\
  \end{bmatrix}
\end{equation*}

\subsubsection*{Stretch of $k$ in the $y$ direction}
\begin{equation*}
  \begin{bmatrix}
    1 & 0 \\
    0 & k \\
  \end{bmatrix}
\end{equation*}
\subsubsection*{Shear of $k$ in the $x$ direction }
\begin{equation*}
  \begin{bmatrix}
    1 & k \\
    0 & 1 \\
  \end{bmatrix}
\end{equation*}

\subsubsection*{Shear of $k$ in the $y$ direction }
\begin{equation*}
  \begin{bmatrix}
    1 & 0 \\
    k & 1 \\
  \end{bmatrix}
\end{equation*}

\subsubsection*{Rotation about the origin by $\theta$ anti-clockwise}
\begin{equation*}
  \begin{bmatrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta) \\
  \end{bmatrix}
\end{equation*}
\end{definition}

\marginnote{
  \subsubsection*{Combinations}
To combine transformations we can multiply transformation matrices. The order that the 
transformations will happen will be the order you multiply them in. i.e $\textbf{A}\textbf{B}$
means transformation \textbf{A} will be applied before \textbf{B}.
}[-16cm]

\subsection*{Determinants}
Determinants are a property of matrices that can help you to understand a matrix transform.
In 2D, it will tell you the ratio of the areas of the matrix and the transformed matrix.
In 3D, it will tell you the ratio of the volumes. 
In other words, `determinants give scale'.
Additionally note, A negative determinant means the area or volume will be inverted.

\newpage

\subsubsection*{Calculating Determinants}
\begin{equation*}
  \det
  \begin{bmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} \\
  \end{bmatrix}
  = a_{11} \times a_{22} - a_{12} \times a_{21} \\
\end{equation*}
\begin{equation*}
  \det
  \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
  \end{bmatrix}
  = a_{11} \times 
  \begin{vmatrix}
    a_{22} & a_{23} \\
    a_{32} & a_{33} \\
  \end{vmatrix}
    - a_{12} \times 
  \begin{vmatrix}
    a_{21} & a_{23} \\
    a_{31} & a_{33} \\
  \end{vmatrix}
    + a_{13} \times
  \begin{vmatrix}
    a_{21} & a_{22} \\
    a_{31} & a_{32} \\
  \end{vmatrix}
\end{equation*}

\subsection*{The Identity Matrix}
The identity matrix \textbf{I} is a matrix that when multiplied with any other matrix will
return that matrix. It is analogous to the scalar `one'. An identity matrix will contain 
a single line of 1s through the leading diagonal and 0s on either side i.e for a 2x2 matrix:
\begin{equation*}
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}
\end{equation*}

\subsection*{Inverses}
An inverse given by $\textbf{A}^{-1}$ reverses the transformation performed by \textbf{A}
hence $\textbf{A}^{-1}$ (if it exists) is the inverse transformation of \textbf{A}. When 
you multiply any inverse $\textbf{A}^{-1}$ by \textbf{A} it should return the identity 
matrix \textbf{I}.
\subsubsection*{Calculating Inverses}
\begin{equation*}
  \begin{bmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} \\
  \end{bmatrix}
  ^{-1}
  =
  \frac{1}{
    \begin{vmatrix}
      a_{11} & a_{12} \\
      a_{21} & a_{22} \\
    \end{vmatrix}
  }
  \begin{bmatrix}
      a_{22} & -a_{12} \\
      -a_{21} & a_{11} \\
  \end{bmatrix}
\end{equation*}

\end{document}
